import torch
from torch import nn
from Network.attention import MultiHeadAttention


class PreNorm(nn.Module):

    def __init__(self, dim, fn):
        """PreNorm module to apply layer normalization before a given function
        :param:
            dim  -> int: Dimension of the input
            fn   -> nn.Module: The function to apply after layer normalization
        """
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        """Forward pass through the PreNorm module
        :param:
            x        -> torch.Tensor: Input tensor
            **kwargs -> _ : Additional keyword arguments for the function
        :return
            torch.Tensor: Output of the function applied after layer normalization
        """
        x = torch.clamp(
            x, torch.finfo(torch.float16).min, torch.finfo(torch.float16).max
        )
        return self.fn(self.norm(x), **kwargs)


class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.0):
        """Initialize the Multi-Layer Perceptron (MLP).
        :param:
            dim        -> int : Dimension of the input
            dim        -> int : Dimension of the hidden layer
            dim        -> float : Dropout rate
        """
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim, bias=True),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim, bias=True),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        """Forward pass through the MLP module.
        :param:
            x -> torch.Tensor: Input tensor
        :return
            torch.Tensor: Output of the function applied after layer
        """
        return self.net(x)


class Attention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.0, rope=False):
        """Initialize the Attention module.
        :param:
            embed_dim     -> int : Dimension of the embedding
            num_heads     -> int : Number of heads
            dropout       -> float : Dropout rate
        """
        super(Attention, self).__init__()
        self.dim = embed_dim
        # self.mha = nn.MultiheadAttention(
        #     embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True, bias=True
        # )
        self.mha = MultiHeadAttention(
            embed_dim,
            num_heads,
            rope=rope,
            grid_shape=(7, 7),
            geometric_base=100,
            dropout=dropout,
        )

    def forward(self, x):
        """Forward pass through the Attention module.
        :param:
            x -> torch.Tensor: Input tensor
        :return
            attention_value  -> torch.Tensor: Output the value of the attention
            attention_weight -> torch.Tensor: Output the weight of the attention
        """
        attention_value, attention_weight = self.mha(x, x, x)
        return attention_value, attention_weight


class TransformerEncoder(nn.Module):
    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.0, rope=False):
        """Initialize the Attention module.
        :param:
            dim       -> int : number of hidden dimension of attention
            depth     -> int : number of layer for the transformer
            heads     -> int : Number of heads
            mlp_dim   -> int : number of hidden dimension for mlp
            dropout   -> float : Dropout rate
        """
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        PreNorm(dim, Attention(dim, heads, dropout=dropout, rope=rope)),
                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)),
                    ]
                )
            )

    def forward(self, x):
        """Forward pass through the Attention module.
        :param:
            x -> torch.Tensor: Input tensor
        :return
            x -> torch.Tensor: Output of the Transformer
            l_attn -> list(torch.Tensor): list of the attention
        """
        l_attn = []
        for attn, ff in self.layers:
            attention_value, attention_weight = attn(x)
            x = attention_value + x
            x = torch.clamp(
                x, torch.finfo(torch.float16).min, torch.finfo(torch.float16).max
            )
            x = ff(x) + x
            x = torch.clamp(
                x, torch.finfo(torch.float16).min, torch.finfo(torch.float16).max
            )
            l_attn.append(attention_weight)
        return x, l_attn
