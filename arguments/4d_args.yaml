run: # relating to the pytorch lightning
  resume: false # whether to resume training from a checkpoint
  resume_path: false # if resume, the path to the .ckpt file
  test_only: false # only run the evaluation() method for a trained checkpoint
  seed: 42 # random seed
logging: # relating to tensorboard logger
  save_dir: "lightning_logs" # path to logging folder
  name: "test" # subdirectory within logging folder
  version: "test" # subdirectory within name folder
trainer: # relating to pytorch trainer module
  max_epochs: 30 # maximum number of epochs to run
  log_every_n_steps: 100 # logging frequency
  accelerator: "cuda" # cpu | cuda | mps
learning: # relating to training of model
  lr: 0.00001 # learning rate
  sched_mode_learning: "arccos" # this refers to the schedule of masking tokens, not the learning rate scheduler
  lambda_ce: 0.0 # loss = lambda_ce * loss_ce + ...
  lambda_empty: 1.0 # loss = lambda_empty * loss_empty + ...
  lambda_filled: 1.0 # loss = lambda_filled * loss_filled + ...
  frame_masking_frac: 0.0
vit: # relating to vision transformer configuration
  depth: 24 # number of attention/ff layers
  heads: 16 # number of heads within each self attention module
  mlp_dim: 3072 # mlp dimension at input and output of model
  dropout: 0.1 # dropout ratio
  mask_value: 1024 # index of token to use as the mask value (should be = codebook_size)
  empty_value: 1025 # index of token to use to represent empty space (should be = codebook_size+1)
  learnable_codebook: false # whether codebook should be a trainable parameter
  predict_logits: true # whether to predict logits directly or whether to predict a vector
  normalise_embeddings: true # whether to normalise the codebook and the mask and empty tokens before input to transformer
  normalise_transformer_output: false # whether to normalise output of transformer before similarity scoring
  pass_through_tokens: false # whether to ignore the transformer and pass tokens directly from input to output
  remove_final_two_layers: false # whether to remove the final two layers (GELU and layer norm) from the transformer
  positional_embedding: "rope" # learned | sinusoidal | rope
  split_spatio_temporal: true # whether to split the attention into spatial and temporal components
  split_spatio: false
  window_attention: false
  window_size: [5, 5, 5]
vqvae: # relating to the source of the tokens
  codebook_path: "/scratch/foo22/Data/Physics_Simulation/intermediate_data/codebook/loss_experiment/l2_wu_rgb_10x_after/codebook.npy" # path to codebook
  codebook_n: 1024 # number of tokens
  hidden_dim: 4 # dimensionality of each token
data: # relating to data module and data loader
  batch_size: 3
  num_workers: 8
  path: "/scratch/foo22/Data/Physics_Simulation/intermediate_data/codebook/loss_experiment/l2_wu_rgb_10x_after/codebook_indices/" # path to files containing sparse token locations
  temporal: true # whether data carry temporal dimension
  conditionalise_dim: -1 # whether to conditionalise 3d data along one dimension. Set to -1 if no conditionalising
  spatial_size: 8 # spatial size of each token set
  temporal_window: 4 # size of temporal window
